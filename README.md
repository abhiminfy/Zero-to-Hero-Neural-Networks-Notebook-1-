# Neural Networks: Zero to Hero â€” Study Notebooks (Learning-in-Public)

A clean, reproducible set of **Jupyter notebooks** following Andrej Karpathyâ€™s *Neural Networks: Zero to Hero* series.  
This repo is designed to be **teachable** (lots of comments, sanity checks, and visuals) and **hackable** (small, modular utilities you can reuse).

> âš ï¸ Educational project inspired by the original series. Not affiliated with Andrej Karpathy. Links to the video series and references are provided below.

---

## ğŸ§­ TL;DR

- Run each notebook top-to-bottom and youâ€™ll build intuition **from scalar calculus to tiny language models**.
- **Minimal dependencies**, CPU-friendly, and **seeded for reproducibility**.
- Each chapter includes:
  - Clear objectives & learning outcomes
  - Experiments & toggles you can try (no extra code needed)
  - Notes on failure modes and how to debug them

---

## ğŸ“š Table of Contents

- [Whatâ€™s in this repo?](#whats-in-this-repo)
- [Repo structure](#repo-structure)
- [Environment & setup](#environment--setup)
- [How to run (VS Code or CLI)](#how-to-run-vs-code-or-cli)
- [Datasets](#datasets)
- [Notebooks & learning outcomes](#notebooks--learning-outcomes)
- [Experiments you can try](#experiments-you-can-try)
- [Reproducibility](#reproducibility)
- [Troubleshooting](#troubleshooting)
- [Roadmap](#roadmap)
- [Acknowledgements](#acknowledgements)
- [References](#references)
- [License](#license)

---

## ğŸ” Whatâ€™s in this repo?

A beginner-friendly walkthrough of core deep learning ideas, implemented **from scratch** and then in **PyTorch** when helpful.  
Youâ€™ll start with gradients and end up sampling text from a tiny **bigram** model (makemore-style) â€” seeing exactly how each piece works.

**Why this repo helps you learn fast:**

- *Small, composable steps*: Each cell does one clear thing.
- *Transparent code*: Heavy inline comments and â€œsanity-checkâ€ prints.
- *Visuals where it matters*: Loss curves, heatmaps, top-k tables.
- *Debug-first mindset*: We log shapes, dtypes, and seed runs.

---

## ğŸ—‚ Repo structure

```
.
â”œâ”€â”€ 01_Micrograd.ipynb                 # Scalar autograd (Value class), backprop from first principles
â”œâ”€â”€ 02_Makemore_Bigram.ipynb           # Bigram language model: counts, smoothing, sampling, trainable weights
â”œâ”€â”€ data/
â”‚   â””â”€â”€ names.txt                      # Simple newline-separated dataset of names (place here)
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ plotting.py                    # (optional) heatmap/loss helpers
â”‚   â””â”€â”€ io.py                          # (optional) tiny I/O helpers
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

> Your local filenames may differ (e.g., `02_Chapter2.ipynb`). The README assumes the logical mapping above.

---

## ğŸ§© Environment & setup

**Python**: 3.9â€“3.11 recommended (CPU-only is fine).

```bash
# create and activate a virtual environment
python -m venv .venv
# Windows (PowerShell)
. .venv/Scripts/Activate.ps1
# macOS/Linux (bash/zsh)
source .venv/bin/activate

# upgrade base tools
python -m pip install --upgrade pip

# install deps
pip install -r requirements.txt
```

**Minimal requirements (`requirements.txt`):**
```
numpy
torch
matplotlib
tqdm
jupyter
pandas
```

> Tip: In VS Code, select the `.venv` kernel (bottom-right of the notebook UI) to avoid â€œmodule not foundâ€ errors.

---

## â–¶ï¸ How to run (VS Code or CLI)

**VS Code Notebook UI**
1. Open the repo folder in VS Code.
2. Open a notebook (e.g., `02_Makemore_Bigram.ipynb`).
3. Pick the `.venv` kernel.
4. Run all cells (â­ï¸).

**CLI (Jupyter)**
```bash
jupyter notebook
# or
jupyter lab
```

---

## ğŸ“ Datasets

Place a simple newline-separated file of names at `data/names.txt`.  
You can use any small text corpus (one item per line). The bigram notebook expects **start/end tokens** to be introduced programmatically; no special markup required.

```text
data/
â””â”€â”€ names.txt
```

---

## ğŸ“’ Notebooks & learning outcomes

### 01 â€” Micrograd (from-scratch autograd) âœ…
**Core ideas**
- Build a `Value` scalar object with a computation graph.
- Implement forward ops (+, *, tanh, etc.) and **manual backward** (reverse-mode autodiff).
- Visualize simple graphs and confirm **dL/dw** by hand vs. code.

**Youâ€™ll be able to:**
- Derive and implement backprop on tiny graphs.
- Inspect gradients at each node.
- Sanity-check gradients with finite differences.

---

### 02 â€” Makemore (Bigram language model) âœ…
**Core ideas**
- Tokenize characters, add **start/end tokens**, build a **VÃ—V bigram count matrix**.
- Apply **Laplace smoothing (Î±)** to avoid zero-probability rows.
- Convert counts â†’ probabilities; **sample** new names one character at a time.
- Train a **learned bigram** in PyTorch with **cross-entropy** and track loss.

**Why it matters**
- Establish a **baseline** for language modeling thatâ€™s easy to debug and extend.
- Build intuition for **n-gram limitations** (no long-range memory).
- Create **interpretable artifacts**: heatmaps, top-k next-char tables.

**Artifacts**
- `bigram_heatmap.png` (optional)
- `samples.txt` (optional)

---

## ğŸ§ª Experiments you can try

- **Smoothing sweep**: vary Î± âˆˆ {0, 0.1, 0.5, 1.0} and compare diversity vs. stability.
- **Start/End tokens**: toggle on/off; inspect how endings improve.
- **Train/Val/Test**: confirm generalization; tune training steps/learning rate.
- **Top-k probes**: for each letter, print most likely next characters.
- **Dataset swap**: plug in a different `names.txt` and re-run to see style shifts.

---

## ğŸ” Reproducibility

We fix seeds for `random`, `numpy`, and `torch`:

```python
import random, numpy as np, torch
random.seed(1337); np.random.seed(1337); torch.manual_seed(1337)
```

> Note: Exact floating-point results can still vary slightly across OS/PyTorch builds.

---

## ğŸ› ï¸ Troubleshooting

- **VS Code shows multiple Python interpreters** â†’ pick the one with `.venv` in its path.
- **`ModuleNotFoundError`** â†’ the notebook isnâ€™t using your venv. Re-select kernel and restart.
- **CUDA warnings** â†’ safe to ignore on CPU. The notebooks are optimized for CPU runs.
- **Different samples than the video** â†’ stochastic sampling + dataset differences. Compare your **bigram heatmap** and **train/val loss** to debug.

---

## ğŸ—ºï¸ Roadmap

- [x] 01 â€” Micrograd: autograd from first principles
- [x] 02 â€” Makemore â€” Bigram (counts, smoothing, sampling, trainable bigram)
- [ ] 03 â€” Makemore â€” MLP (embedding + simple neural LM)
- [ ] 04 â€” Recurrent nets (RNN/LSTM intuition)
- [ ] 05 â€” Attention & Transformers (makemore final)
- [ ] 06 â€” Extras (tokenization tricks, batching, efficiency)

> The roadmap mirrors the video series at a high level but prioritizes **clarity** and **small steps**.

---

## ğŸ™ Acknowledgements

- Andrej Karpathy for *Neural Networks: Zero to Hero* â€” the inspiration for this learning journey.
- **Minfy** and **Blesson Davis** for encouraging a learning-in-public culture and supporting the time to build/share these materials.

---

## ğŸ”— References

- *Neural Networks: Zero to Hero* â€” Andrej Karpathy (YouTube series)
- *makemore* repository and related educational materials

---

## ğŸ“ License

This project is for educational purposes. Youâ€™re welcome to fork and adapt it; credit is appreciated.


